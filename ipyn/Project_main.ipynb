{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert category name to wiki API friendly\n",
    "def format_cat_name(cat_name): \n",
    "    cat_name = re.sub('\\s','_', cat_name)\n",
    "    return cat_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fucntion to build up the query to search for category in wiki api\n",
    "def go_query(cat_name):\n",
    "    cate_name = format_cat_name(cat_name)\n",
    "    params = {\"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"cmtitle\": cate_name,\n",
    "            \"cmlimit\": \"max\"}\n",
    "    query = requests.get(\"http://en.wikipedia.org/w/api.php?\",params=params)\n",
    "    \n",
    "    return query.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe from json format obtained from wiki api\n",
    "def json_df(cat_name):\n",
    "    temp_dict = go_query(cat_name)\n",
    "    df = pd.DataFrame(temp_dict['query']['categorymembers'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to obtain pages in passed category name with three recursive to get to sub categories\n",
    "def cat_pages(cat_name, max_depth=3):\n",
    "    \n",
    "    params = {'action':'query',\n",
    "          'format':'json',\n",
    "          'list':'categorymembers',\n",
    "          'cmtitle': format_cat_name(cat_name),\n",
    "          'cmlimit':'max'}\n",
    "    \n",
    "    \n",
    "    response = requests.get('http://en.wikipedia.org/w/api.php?', params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    members = data['query']['categorymembers']\n",
    "\n",
    "    pages = list(filter(lambda x: x['ns'] == 0, members))\n",
    "    subpages = list(filter(lambda x: x['ns'] == 14, members))\n",
    "      \n",
    "    while max_depth > 0:\n",
    " \n",
    "        if not subpages:    \n",
    "            return pages\n",
    "    \n",
    "        else:\n",
    "            max_depth -= 1\n",
    "            for subpage in subpages:\n",
    "                pages += cat_pages(subpage['title'], max_depth)\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of page names\n",
    "def page_list(cat_name):\n",
    "    pages = cat_pages(format_cat_name(cat_name))\n",
    "    pages_df = pd.DataFrame(pages)\n",
    "    page_list = list(pages_df['title'])\n",
    "    return page_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining contents of the page by passing a category name, return the content not dataframe\n",
    "def get_content(cat_name):\n",
    "    params = {'action':'query',\n",
    "          'titles':format_cat_name(cat_name),\n",
    "          'prop':'extracts',\n",
    "          'rvprop': 'content',\n",
    "          'format':'json'}\n",
    "    \n",
    "    response = requests.get('http://en.wikipedia.org/w/api.php?', params=params)\n",
    "    data = response.json()\n",
    "    return_data = data['query']['pages']\n",
    "    \n",
    "    page_id = list(return_data.keys())[0]\n",
    "    content = return_data[page_id]['extract']\n",
    "    title = format_cat_name(cat_name)\n",
    "    \n",
    "    content_df = pd.DataFrame([page_id, title, content],index=(['page_id', 'title', 'content'])).T\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create Dataframe with page_id, title, content by sentences\n",
    "def get_content_df_sen(cat_name):\n",
    "    params = {'action':'query',\n",
    "          'titles':format_cat_name(cat_name),\n",
    "          'prop':'extracts',\n",
    "          'rvprop': 'content',\n",
    "          'format':'json'}\n",
    "    \n",
    "    response = requests.get('http://en.wikipedia.org/w/api.php?', params=params)\n",
    "    data = response.json()\n",
    "    return_data = data['query']['pages']\n",
    "    \n",
    "    page_id = list(return_data.keys())[0]\n",
    "    content = return_data[page_id]['extract']\n",
    "    soup = BeautifulSoup(content,\"html5lib\")\n",
    "    \n",
    "    temp_list=[]\n",
    "    for string in soup.stripped_strings:\n",
    "        temp_list.append(string)\n",
    "    \n",
    "    #s=''\n",
    "    #clean = s.join(temp_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    title = format_cat_name(cat_name)\n",
    "    \n",
    "    content_df = pd.DataFrame([page_id, title, clean],index=(['page_id', 'title', 'content'])).T\n",
    "    \n",
    "    return content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up the syntex of content by BeautifulSoup\n",
    "def clean_content(cat_name):\n",
    "    page = get_content(cat_name) \n",
    "    soup = BeautifulSoup(page, \"html5lib\")\n",
    "    temp_list=[]\n",
    "    for string in soup.stripped_strings:\n",
    "        temp_list.append(string)\n",
    "    \n",
    "    \n",
    "    s=''\n",
    "    clean = s.join(temp_list)\n",
    "\n",
    "    return clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_df(cat_name):\n",
    "    params = {'action':'query',\n",
    "          'titles':format_cat_name(cat_name),\n",
    "          'prop':'extracts',\n",
    "          'rvprop': 'content',\n",
    "          'format':'json'}\n",
    "    \n",
    "    response = requests.get('http://en.wikipedia.org/w/api.php?', params=params)\n",
    "    data = response.json()\n",
    "    return_data = data['query']['pages']\n",
    "    \n",
    "    page_id = list(return_data.keys())[0]\n",
    "    content = return_data[page_id]['extract']\n",
    "    soup = BeautifulSoup(content,\"html5lib\")\n",
    "    \n",
    "    temp_str=str()\n",
    "    for string in soup.stripped_strings:\n",
    "        temp_str += string \n",
    "    \n",
    "    \n",
    "    #clean = str(temp_list)\n",
    "    \n",
    "    title = format_cat_name(cat_name)\n",
    "    \n",
    "    content_df = pd.DataFrame([page_id, title, temp_str],index=(['page_id', 'title', 'content'])).T\n",
    "    \n",
    "    \n",
    "    return content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mongo client IP\n",
    "client = MongoClient('54.190.53.213', 27016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wiki Mongo reference for Machine Learning content by sentences\n",
    "db_ref = client.my_database\n",
    "db_wiki_ref = db_ref.my_wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wiki Mongo reference for Business Software content by sentences\n",
    "db_wiki_bs_ref = db_ref.my_wiki_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wiki Mongo reference for Machie Learning content whole text\n",
    "db_wiki_whole_ref = db_ref.my_wikipedia_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wiki Mongo reference for Business Software content whole text\n",
    "db_wiki_bs_whole_ref = db_ref.my_wiki_bs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['admin', 'local', 'my_database', 'test'],\n",
       " ['my_collection',\n",
       "  'my_wikipedia_all',\n",
       "  'my_wikipedia',\n",
       "  'my_wiki_bs',\n",
       "  'my_wiki_bs_all'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check names of the Mongo Database Structure\n",
    "client.database_names(), db_ref.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wiki Collection Process for Machine Learning Categories\n",
    "\n",
    "#Create list with set so duplicates are gone\n",
    "ml_page_list = set(page_list(\"Category:Machine learning\"))\n",
    "\n",
    "#Store cleaned contents in the list \n",
    "content_list=[]\n",
    "for title in ml_page_list:\n",
    "    content_list.append(clean_content(title))\n",
    "\n",
    "#Replcae '.' with space since Mongo deosn't like keys with periods \n",
    "ml_page_list_2 = [x.replace('.',' ') for x in ml_page_list]\n",
    "\n",
    "#Create a list of dictionaries of Title:content\n",
    "new_list = []\n",
    "for i in range(len(ml_page_list_2)):\n",
    "    new_dict = {ml_page_list_2[i]:content_list[i]}\n",
    "    new_list.append(new_dict)\n",
    "\n",
    "#Store list of dictionaries to Mongo\n",
    "for i in new_list:\n",
    "    db_wiki_ref.insert_one(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retireving list of dictionaries \n",
    "ml_dict = list(db_wiki_ref.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Melomics': ['Melomics',\n",
       "   '(derived from \"genomics of melodies\") is a computational system for the automatic composition of music (with no human intervention), based on bioinspired algorithms.',\n",
       "   'Technological aspects',\n",
       "   'Melomics applies an evolutionary approach to music composition, i.e., music pieces are obtained by simulated evolution. These themes compete to better adapt to a proper fitness function, generally grounded on formal and aesthetic criteria. The Melomics system encodes each theme in a genome, and the entire population of music pieces undergoes evo-devo dynamics (i.e., pieces read-out mimicking a complex embryological development process). The system is fully autonomous: once programmed, it composes music without human intervention.',\n",
       "   \"This technology has been transferred to industry as an academic spin-off, Melomics Media, which has provided and reprogrammed a new computer cluster that created a huge collection of popular music. The results of this evolutionary computation are being stored in Melomics' site, which nowadays constitutes a vast repository of music content. A differentiating feature is that pieces are available in three types of formats: playable (MP3), editable (MIDI and MusicXML) and readable (score in PDF).\",\n",
       "   'Computer clusters',\n",
       "   'The Melomics computational system includes two computer clusters: Melomics109 and Iamus, dedicated to popular and artistic music, respectively.',\n",
       "   'Melomics109: The largest repository of popular music',\n",
       "   'Melomics109 is cluster programmed and integrated in the Melomics system. Its first product is a vast repository of popular music compositions (roughly 1 billion), covering all essential styles. In addition to MP3, all songs are available in editable formats (MIDI); and music is licensed under CC0, meaning that it is freely downloadable.',\n",
       "   '0music is the first album published by Melomics109, which is available in MP3 and MIDI formats, under CC0 license.',\n",
       "   'It has been argued that, by making such amount of editable, original and royalty-free music accessible to people, Melomics may accelerate the process of commoditization of music, and change the way music is composed and consumed in the future.',\n",
       "   'Iamus: First album of professional contemporary music by a non-human intelligence',\n",
       "   'In the first stages of the development of the Melomics system, Iamus composed',\n",
       "   'Opus one',\n",
       "   '(on October 15, 2010), arguably the first fragment of professional contemporary classical music ever composed by a computer in its own style, rather than attempting to emulate the style of existing composers. The first full composition (also in contemporary classic style),',\n",
       "   'Hello World!',\n",
       "   ', premiered exactly one year after the creation of',\n",
       "   'Opus one',\n",
       "   ', on October 15, 2011. Four later works premiered on July 2, 2012, and were broadcast live from the School of Computer Science at Universidad de Málaga as part of the events included in the Alan Turing year. The compositions performed at this event were later recorded by the London Symphony Orchestra, creating Iamus’ eponymous first album, which New Scientist reported as the \"first complete album to be composed solely by a computer and recorded by human musicians.\"',\n",
       "   'Commenting on the quality and authenticity of the music, Stephen Smoliar, critic of classical music at',\n",
       "   'The San Francisco Examiner',\n",
       "   ', commented \"What is primary is the act of making the music itself engaged by the performers and how the listener responds to what those performers do... what is most interesting about the documents generated by Iamus is their capacity to challenge the creative talents of performing musicians\".',\n",
       "   'Applications',\n",
       "   \"Melomics' empathic music has been tested in a number of therapeutic clinical trials, evidencing positive effects in reducing fear of heights, acute stress and pain perception. One of the studies resulted in a reduction of almost two thirds of pain perception in children's prick test, as compared to the standard procedure. Some of these experiments made use of mobile free apps to adapt music to daily activity, like jogging, or commuting, but also for therapeutic use, like lessen stress before an exam, for chronic pain, insomnia, and to help children to initiate sleep.\",\n",
       "   'The way Melomics can adapt music in real-time to the physiological evolution of the listener, and to music branding has also been reported.',\n",
       "   'External links',\n",
       "   'Melomics Homepage',\n",
       "   'Melomics page at University of Malaga (Spain)',\n",
       "   'References'],\n",
       "  '_id': ObjectId('5a21fd9e1e6b1f001c45d46e')},\n",
       " {'Weighted majority algorithm (machine learning)': ['In machine learning,',\n",
       "   'weighted majority algorithm (WMA)',\n",
       "   'is a meta-learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts. The algorithm assumes that we have no prior knowledge about the accuracy of the algorithms in the pool, but there are sufficient reasons to believe that one or more will perform well.',\n",
       "   'Assume that the problem is a binary decision problem. To construct the compound algorithm, a positive weight is given to each of the algorithms in the pool. The compound algorithm then collects weighted votes from all the algorithms in the pool, and gives the prediction that has a higher vote. If the compound algorithm makes a mistake, the algorithms in the pool that contributed to the wrong predicting will be discounted by a certain ratio β where 0<β<1.',\n",
       "   'It can be shown that the upper bounds on the number of mistakes made in a given sequence of predictions from a pool of algorithms',\n",
       "   'A',\n",
       "   '{\\\\displaystyle \\\\mathbf {A} }',\n",
       "   'is',\n",
       "   'O',\n",
       "   '(',\n",
       "   'l',\n",
       "   'o',\n",
       "   'g',\n",
       "   '|',\n",
       "   'A',\n",
       "   '|',\n",
       "   '+',\n",
       "   'm',\n",
       "   ')',\n",
       "   '{\\\\displaystyle \\\\mathbf {O(log|A|+m)} }',\n",
       "   'if one algorithm in',\n",
       "   'x',\n",
       "   'i',\n",
       "   '{\\\\displaystyle \\\\mathbf {x} _{i}}',\n",
       "   'makes at most',\n",
       "   'm',\n",
       "   '{\\\\displaystyle \\\\mathbf {m} }',\n",
       "   'mistakes.',\n",
       "   'There are many variations of the weighted majority algorithm to handle different situations, like shifting targets, infinite pools, or randomized predictions. The core mechanism remain similar, with the final performances of the compound algorithm bounded by a function of the performance of the',\n",
       "   'specialist',\n",
       "   '(best performing algorithm) in the pool.',\n",
       "   'See also',\n",
       "   'Randomized weighted majority algorithm',\n",
       "   'References'],\n",
       "  '_id': ObjectId('5a21fd9e1e6b1f001c45d46f')},\n",
       " {'K-nearest neighbors algorithm': ['In pattern recognition, the',\n",
       "   'k',\n",
       "   '-nearest neighbors algorithm',\n",
       "   '(',\n",
       "   'k',\n",
       "   '-NN',\n",
       "   ') is a non-parametric method used for classification and regression. In both cases, the input consists of the',\n",
       "   'k',\n",
       "   'closest training examples in the feature space. The output depends on whether',\n",
       "   'k',\n",
       "   '-NN is used for classification or regression:',\n",
       "   'In',\n",
       "   'k-NN classification',\n",
       "   ', the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its',\n",
       "   'k',\n",
       "   'nearest neighbors (',\n",
       "   'k',\n",
       "   'is a positive integer, typically small). If',\n",
       "   'k',\n",
       "   '=\\xa01, then the object is simply assigned to the class of that single nearest neighbor.',\n",
       "   'In',\n",
       "   'k-NN regression',\n",
       "   ', the output is the property value for the object. This value is the average of the values of its',\n",
       "   'k',\n",
       "   'nearest neighbors.',\n",
       "   'k',\n",
       "   '-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The',\n",
       "   'k',\n",
       "   '-NN algorithm is among the simplest of all machine learning algorithms.',\n",
       "   'Both for classification and regression, a useful technique can be to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/',\n",
       "   'd',\n",
       "   ', where',\n",
       "   'd',\n",
       "   'is the distance to the neighbor.',\n",
       "   'The neighbors are taken from a set of objects for which the class (for',\n",
       "   'k',\n",
       "   '-NN classification) or the object property value (for',\n",
       "   'k',\n",
       "   '-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.',\n",
       "   'A peculiarity of the',\n",
       "   'k',\n",
       "   '-NN algorithm is that it is sensitive to the local structure of the data. The algorithm is not to be confused with',\n",
       "   'k',\n",
       "   '-means, another popular machine learning technique.',\n",
       "   'Statistical setting',\n",
       "   'Suppose we have pairs',\n",
       "   '(',\n",
       "   'X',\n",
       "   ',',\n",
       "   'Y',\n",
       "   ')',\n",
       "   ',',\n",
       "   '(',\n",
       "   'X',\n",
       "   '1',\n",
       "   ',',\n",
       "   'Y',\n",
       "   '1',\n",
       "   ')',\n",
       "   ',',\n",
       "   '…',\n",
       "   ',',\n",
       "   '(',\n",
       "   'X',\n",
       "   'n',\n",
       "   ',',\n",
       "   'Y',\n",
       "   'n',\n",
       "   ')',\n",
       "   '{\\\\displaystyle (X,Y),(X_{1},Y_{1}),\\\\dots ,(X_{n},Y_{n})}',\n",
       "   'taking values in',\n",
       "   'R',\n",
       "   'd',\n",
       "   '×',\n",
       "   '{',\n",
       "   '1',\n",
       "   ',',\n",
       "   '2',\n",
       "   '}',\n",
       "   '{\\\\displaystyle \\\\mathbb {R} ^{d}\\\\times \\\\{1,2\\\\}}',\n",
       "   ', where',\n",
       "   'Y',\n",
       "   'is the class label of',\n",
       "   'X',\n",
       "   ', so that',\n",
       "   'X',\n",
       "   '|',\n",
       "   'Y',\n",
       "   '=',\n",
       "   'r',\n",
       "   '∼',\n",
       "   'P',\n",
       "   'r',\n",
       "   '{\\\\displaystyle X|Y=r\\\\sim P_{r}}',\n",
       "   'for',\n",
       "   'r',\n",
       "   '=',\n",
       "   '1',\n",
       "   ',',\n",
       "   '2',\n",
       "   '{\\\\displaystyle r=1,2}',\n",
       "   '(and probability distributions',\n",
       "   'P',\n",
       "   'r',\n",
       "   '{\\\\displaystyle P_{r}}',\n",
       "   '). Given some norm',\n",
       "   '∥',\n",
       "   '⋅',\n",
       "   '∥',\n",
       "   '{\\\\displaystyle \\\\|\\\\cdot \\\\|}',\n",
       "   'on',\n",
       "   'R',\n",
       "   'd',\n",
       "   '{\\\\displaystyle \\\\mathbb {R} ^{d}}',\n",
       "   'and a point',\n",
       "   'x',\n",
       "   '∈',\n",
       "   'R',\n",
       "   'd',\n",
       "   '{\\\\displaystyle x\\\\in \\\\mathbb {R} ^{d}}',\n",
       "   ', let',\n",
       "   '(',\n",
       "   'X',\n",
       "   '(',\n",
       "   '1',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Y',\n",
       "   '(',\n",
       "   '1',\n",
       "   ')',\n",
       "   ')',\n",
       "   ',',\n",
       "   '…',\n",
       "   ',',\n",
       "   '(',\n",
       "   'X',\n",
       "   '(',\n",
       "   'n',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Y',\n",
       "   '(',\n",
       "   'n',\n",
       "   ')',\n",
       "   ')',\n",
       "   '{\\\\displaystyle (X_{(1)},Y_{(1)}),\\\\dots ,(X_{(n)},Y_{(n)})}',\n",
       "   'be a reordering of the training data such that',\n",
       "   '∥',\n",
       "   'X',\n",
       "   '(',\n",
       "   '1',\n",
       "   ')',\n",
       "   '−',\n",
       "   'x',\n",
       "   '∥',\n",
       "   '≤',\n",
       "   '⋯',\n",
       "   '≤',\n",
       "   '∥',\n",
       "   'X',\n",
       "   '(',\n",
       "   'n',\n",
       "   ')',\n",
       "   '−',\n",
       "   'x',\n",
       "   '∥',\n",
       "   '{\\\\displaystyle \\\\|X_{(1)}-x\\\\|\\\\leq \\\\dots \\\\leq \\\\|X_{(n)}-x\\\\|}',\n",
       "   '.',\n",
       "   'Algorithm',\n",
       "   'The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.',\n",
       "   'In the classification phase,',\n",
       "   'k',\n",
       "   'is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the',\n",
       "   'k',\n",
       "   'training samples nearest to that query point.',\n",
       "   'A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the',\n",
       "   'overlap metric',\n",
       "   '(or Hamming distance). In the context of gene expression microarray data, for example,',\n",
       "   'k',\n",
       "   '-NN has also been employed with correlation coefficients such as Pearson and Spearman. Often, the classification accuracy of',\n",
       "   'k',\n",
       "   '-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.',\n",
       "   'A drawback of the basic \"majority voting\" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the',\n",
       "   'k',\n",
       "   'nearest neighbors due to their large number. One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its',\n",
       "   'k',\n",
       "   'nearest neighbors. The class (or value, in regression problems) of each of the',\n",
       "   'k',\n",
       "   'nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a self-organizing map (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data.',\n",
       "   'K',\n",
       "   '-NN can then be applied to the SOM.',\n",
       "   'Parameter selection',\n",
       "   'The best choice of',\n",
       "   'k',\n",
       "   'depends upon the data; generally, larger values of',\n",
       "   'k',\n",
       "   'reduce the effect of noise on the classification, but make boundaries between classes less distinct. A good',\n",
       "   'k',\n",
       "   'can be selected by various heuristic techniques (see hyperparameter optimization). The special case where the class is predicted to be the class of the closest training sample (i.e. when',\n",
       "   'k',\n",
       "   '= 1) is called the nearest neighbor algorithm.',\n",
       "   'The accuracy of the',\n",
       "   'k',\n",
       "   '-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into selecting or scaling features to improve classification. A particularly popular approach is the use of evolutionary algorithms to optimize feature scaling. Another popular approach is to scale features by the mutual information of the training data with the training classes.',\n",
       "   'In binary (two class) classification problems, it is helpful to choose',\n",
       "   'k',\n",
       "   'to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal',\n",
       "   'k',\n",
       "   'in this setting is via bootstrap method.',\n",
       "   'The',\n",
       "   '1',\n",
       "   '-nearest neighbour classifier',\n",
       "   'The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point',\n",
       "   'x',\n",
       "   'to the class of its closest neighbour in the feature space, that is',\n",
       "   'C',\n",
       "   'n',\n",
       "   '1',\n",
       "   'n',\n",
       "   'n',\n",
       "   '(',\n",
       "   'x',\n",
       "   ')',\n",
       "   '=',\n",
       "   'Y',\n",
       "   '(',\n",
       "   '1',\n",
       "   ')',\n",
       "   '{\\\\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}',\n",
       "   '.',\n",
       "   'As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data).',\n",
       "   'The weighted nearest neighbour classifier',\n",
       "   'The',\n",
       "   'k',\n",
       "   '-nearest neighbour classifier can be viewed as assigning the',\n",
       "   'k',\n",
       "   'nearest neighbours a weight',\n",
       "   '1',\n",
       "   '/',\n",
       "   'k',\n",
       "   '{\\\\displaystyle 1/k}',\n",
       "   'and all others',\n",
       "   '0',\n",
       "   'weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the',\n",
       "   'i',\n",
       "   'th nearest neighbour is assigned a weight',\n",
       "   'w',\n",
       "   'n',\n",
       "   'i',\n",
       "   '{\\\\displaystyle w_{ni}}',\n",
       "   ', with',\n",
       "   '∑',\n",
       "   'i',\n",
       "   '=',\n",
       "   '1',\n",
       "   'n',\n",
       "   'w',\n",
       "   'n',\n",
       "   'i',\n",
       "   '=',\n",
       "   '1',\n",
       "   '{\\\\displaystyle \\\\sum _{i=1}^{n}w_{ni}=1}',\n",
       "   '. An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds.',\n",
       "   'Let',\n",
       "   'C',\n",
       "   'n',\n",
       "   'w',\n",
       "   'n',\n",
       "   'n',\n",
       "   '{\\\\displaystyle C_{n}^{wnn}}',\n",
       "   'denote the weighted nearest classifier with weights',\n",
       "   '{',\n",
       "   'w',\n",
       "   'n',\n",
       "   'i',\n",
       "   '}',\n",
       "   'i',\n",
       "   '=',\n",
       "   '1',\n",
       "   'n',\n",
       "   '{\\\\displaystyle \\\\{w_{ni}\\\\}_{i=1}^{n}}',\n",
       "   '. Subject to regularity conditions on to class distributions the excess risk has the following asymptotic expansion',\n",
       "   'R',\n",
       "   'R',\n",
       "   '(',\n",
       "   'C',\n",
       "   'n',\n",
       "   'w',\n",
       "   'n',\n",
       "   'n',\n",
       "   ')',\n",
       "   '−',\n",
       "   'R',\n",
       "   'R',\n",
       "   '(',\n",
       "   'C',\n",
       "   'B',\n",
       "   'a',\n",
       "   'y',\n",
       "   'e',\n",
       "   's',\n",
       "   ')',\n",
       "   '=',\n",
       "   '(',\n",
       "   'B',\n",
       "   '1',\n",
       "   's',\n",
       "   'n',\n",
       "   '2',\n",
       "   '+',\n",
       "   'B',\n",
       "   '2',\n",
       "   't',\n",
       "   'n',\n",
       "   '2',\n",
       "   ')',\n",
       "   '{',\n",
       "   '1',\n",
       "   '+',\n",
       "   'o',\n",
       "   '(',\n",
       "   '1',\n",
       "   ')',\n",
       "   '}',\n",
       "   ',',\n",
       "   '{\\\\displaystyle {\\\\mathcal {R}}_{\\\\mathcal {R}}(C_{n}^{wnn})-{\\\\mathcal {R}}_{\\\\mathcal {R}}(C^{Bayes})=\\\\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\\\\right)\\\\{1+o(1)\\\\},}',\n",
       "   'for constants',\n",
       "   'B',\n",
       "   '1',\n",
       "   '{\\\\displaystyle B_{1}}',\n",
       "   'and',\n",
       "   'B',\n",
       "   '2',\n",
       "   '{\\\\displaystyle B_{2}}',\n",
       "   'where',\n",
       "   's',\n",
       "   'n',\n",
       "   '2',\n",
       "   '=',\n",
       "   '∑',\n",
       "   'i',\n",
       "   '=',\n",
       "   '1',\n",
       "   'n',\n",
       "   'w',\n",
       "   'n',\n",
       "   'i',\n",
       "   '2',\n",
       "   '{\\\\displaystyle s_{n}^{2}=\\\\sum _{i=1}^{n}w_{ni}^{2}}',\n",
       "   'and',\n",
       "   't',\n",
       "   'n',\n",
       "   '=',\n",
       "   'n',\n",
       "   '−',\n",
       "   '2',\n",
       "   '/',\n",
       "   'd',\n",
       "   '∑',\n",
       "   'i',\n",
       "   '=',\n",
       "   '1',\n",
       "   'n',\n",
       "   'w',\n",
       "   'n',\n",
       "   'i',\n",
       "   '{',\n",
       "   'i',\n",
       "   '1',\n",
       "   '+',\n",
       "   '2',\n",
       "   '/',\n",
       "   'd',\n",
       "   '−',\n",
       "   '(',\n",
       "   'i',\n",
       "   '−',\n",
       "   '1',\n",
       "   ')',\n",
       "   '1',\n",
       "   '+',\n",
       "   '2',\n",
       "   '/',\n",
       "   'd',\n",
       "   '}',\n",
       "   '{\\\\displaystyle t_{n}=n^{-2/d}\\\\sum _{i=1}^{n}w_{ni}\\\\{i^{1+2/d}-(i-1)^{1+2/d}\\\\}}',\n",
       "   '.',\n",
       "   'The optimal weighting scheme',\n",
       "   '{',\n",
       "   'w',\n",
       "   'n',\n",
       "   'i',\n",
       "   '∗',\n",
       "   '}',\n",
       "   'i',\n",
       "   '=',\n",
       "   '1',\n",
       "   'n',\n",
       "   '{\\\\displaystyle \\\\{w_{ni}^{*}\\\\}_{i=1}^{n}}',\n",
       "   ', that balances the two terms in the display above, is given as follows: set',\n",
       "   'k',\n",
       "   '∗',\n",
       "   '=',\n",
       "   '⌊',\n",
       "   'B',\n",
       "   'n',\n",
       "   '4',\n",
       "   'd',\n",
       "   '+',\n",
       "   '4',\n",
       "   '⌋',\n",
       "   '{\\\\displaystyle k^{*}=\\\\lfloor Bn^{\\\\frac {4}{d+4}}\\\\rfloor }',\n",
       "   ',',\n",
       "   'w',\n",
       "   'n',\n",
       "   'i',\n",
       "   '∗',\n",
       "   '=',\n",
       "   '1',\n",
       "   'k',\n",
       "   '∗',\n",
       "   '[',\n",
       "   '1',\n",
       "   '+',\n",
       "   'd',\n",
       "   '2',\n",
       "   '−',\n",
       "   'd',\n",
       "   '2',\n",
       "   'k',\n",
       "   '∗',\n",
       "   '2',\n",
       "   '/',\n",
       "   'd',\n",
       "   '{',\n",
       "   'i',\n",
       "   '1',\n",
       "   '+',\n",
       "   '2',\n",
       "   '/',\n",
       "   'd',\n",
       "   '−',\n",
       "   '(',\n",
       "   'i',\n",
       "   '−',\n",
       "   '1',\n",
       "   ')',\n",
       "   '1',\n",
       "   '+',\n",
       "   '2',\n",
       "   '/',\n",
       "   'd',\n",
       "   '}',\n",
       "   ']',\n",
       "   '{\\\\displaystyle w_{ni}^{*}={\\\\frac {1}{k^{*}}}\\\\left[1+{\\\\frac {d}{2}}-{\\\\frac {d}{2{k^{*}}^{2/d}}}\\\\{i^{1+2/d}-(i-1)^{1+2/d}\\\\}\\\\right]}',\n",
       "   'for',\n",
       "   'i',\n",
       "   '=',\n",
       "   '1',\n",
       "   ',',\n",
       "   '2',\n",
       "   ',',\n",
       "   '…',\n",
       "   ',',\n",
       "   'k',\n",
       "   '∗',\n",
       "   '{\\\\displaystyle i=1,2,\\\\dots ,k^{*}}',\n",
       "   'and',\n",
       "   'w',\n",
       "   'n',\n",
       "   'i',\n",
       "   '∗',\n",
       "   '=',\n",
       "   '0',\n",
       "   '{\\\\displaystyle w_{ni}^{*}=0}',\n",
       "   'for',\n",
       "   'i',\n",
       "   '=',\n",
       "   'k',\n",
       "   '∗',\n",
       "   '+',\n",
       "   '1',\n",
       "   ',',\n",
       "   '…',\n",
       "   ',',\n",
       "   'n',\n",
       "   '{\\\\displaystyle i=k^{*}+1,\\\\dots ,n}',\n",
       "   '.',\n",
       "   'With optimal weights the dominant term in the asymptotic expansion of the excess risk is',\n",
       "   'O',\n",
       "   '(',\n",
       "   'n',\n",
       "   '−',\n",
       "   '4',\n",
       "   'd',\n",
       "   '+',\n",
       "   '4',\n",
       "   ')',\n",
       "   '{\\\\displaystyle {\\\\mathcal {O}}(n^{-{\\\\frac {4}{d+4}}})}',\n",
       "   '. Similar results are true when using a bagged nearest neighbour classifier.',\n",
       "   'Properties',\n",
       "   'k',\n",
       "   '-NN is a special case of a variable-bandwidth, kernel density \"balloon\" estimator with a uniform kernel.',\n",
       "   'The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate nearest neighbor search algorithm makes',\n",
       "   'k-',\n",
       "   'NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed.',\n",
       "   'k-',\n",
       "   'NN has some strong consistency results. As the amount of data approaches infinity, the two-class',\n",
       "   'k-',\n",
       "   'NN algorithm is guaranteed to yield an error rate no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data). Various improvements to the',\n",
       "   'k',\n",
       "   '-NN speed are possible by using proximity graphs.',\n",
       "   'For multi-class',\n",
       "   'k-',\n",
       "   'NN classification, Cover and Hart (1967) prove an upper bound error rate of',\n",
       "   'R',\n",
       "   '∗',\n",
       "   '≤',\n",
       "   'R',\n",
       "   'k',\n",
       "   'N',\n",
       "   'N',\n",
       "   '≤',\n",
       "   'R',\n",
       "   '∗',\n",
       "   '(',\n",
       "   '2',\n",
       "   '−',\n",
       "   'M',\n",
       "   'R',\n",
       "   '∗',\n",
       "   'M',\n",
       "   '−',\n",
       "   '1',\n",
       "   ')',\n",
       "   '{\\\\displaystyle R^{*}\\\\ \\\\leq \\\\ R_{k\\\\mathrm {NN} }\\\\ \\\\leq \\\\ R^{*}\\\\left(2-{\\\\frac {MR^{*}}{M-1}}\\\\right)}',\n",
       "   'where',\n",
       "   'R',\n",
       "   '∗',\n",
       "   '{\\\\displaystyle R^{*}}',\n",
       "   'is the Bayes error rate (which is the minimal error rate possible),',\n",
       "   'R',\n",
       "   'k',\n",
       "   'N',\n",
       "   'N',\n",
       "   '{\\\\displaystyle R_{kNN}}',\n",
       "   'is the',\n",
       "   'k-',\n",
       "   'NN error rate, and',\n",
       "   'M',\n",
       "   'is the number of classes in the problem. For',\n",
       "   'M',\n",
       "   '=',\n",
       "   '2',\n",
       "   '{\\\\displaystyle M=2}',\n",
       "   'and as the Bayesian error rate',\n",
       "   'R',\n",
       "   '∗',\n",
       "   '{\\\\displaystyle R^{*}}',\n",
       "   'approaches zero, this limit reduces to \"not more than twice the Bayesian error rate\".',\n",
       "   'Error rates',\n",
       "   'There are many results on the error rate of the',\n",
       "   'k',\n",
       "   'nearest neighbour classifiers. The',\n",
       "   'k',\n",
       "   '-nearest neighbour classifier is strongly (that is for any joint distribution on',\n",
       "   '(',\n",
       "   'X',\n",
       "   ',',\n",
       "   'Y',\n",
       "   ')',\n",
       "   '{\\\\displaystyle (X,Y)}',\n",
       "   ') consistent provided',\n",
       "   'k',\n",
       "   ':=',\n",
       "   'k',\n",
       "   'n',\n",
       "   '{\\\\displaystyle k:=k_{n}}',\n",
       "   'diverges and',\n",
       "   'k',\n",
       "   'n',\n",
       "   '/',\n",
       "   'n',\n",
       "   '{\\\\displaystyle k_{n}/n}',\n",
       "   'converges to zero as',\n",
       "   'n',\n",
       "   '→',\n",
       "   '∞',\n",
       "   '{\\\\displaystyle n\\\\to \\\\infty }',\n",
       "   '.',\n",
       "   'Let',\n",
       "   'C',\n",
       "   'n',\n",
       "   'k',\n",
       "   'n',\n",
       "   'n',\n",
       "   '{\\\\displaystyle C_{n}^{knn}}',\n",
       "   'denote the',\n",
       "   'k',\n",
       "   'nearest neighbour classifier based on a training set of size',\n",
       "   'n',\n",
       "   '. Under certain regularity conditions, the excess risk yields the following asymptotic expansion',\n",
       "   'R',\n",
       "   'R',\n",
       "   '(',\n",
       "   'C',\n",
       "   'n',\n",
       "   'k',\n",
       "   'n',\n",
       "   'n',\n",
       "   ')',\n",
       "   '−',\n",
       "   'R',\n",
       "   'R',\n",
       "   '(',\n",
       "   'C',\n",
       "   'B',\n",
       "   'a',\n",
       "   'y',\n",
       "   'e',\n",
       "   's',\n",
       "   ')',\n",
       "   '=',\n",
       "   '{',\n",
       "   'B',\n",
       "   '1',\n",
       "   '1',\n",
       "   'k',\n",
       "   '+',\n",
       "   'B',\n",
       "   '2',\n",
       "   '(',\n",
       "   'k',\n",
       "   'n',\n",
       "   ')',\n",
       "   '4',\n",
       "   '/',\n",
       "   'd',\n",
       "   '}',\n",
       "   '{',\n",
       "   '1',\n",
       "   '+',\n",
       "   'o',\n",
       "   '(',\n",
       "   '1',\n",
       "   ')',\n",
       "   '}',\n",
       "   ',',\n",
       "   '{\\\\displaystyle {\\\\mathcal {R}}_{\\\\mathcal {R}}(C_{n}^{knn})-{\\\\mathcal {R}}_{\\\\mathcal {R}}(C^{Bayes})=\\\\left\\\\{B_{1}{\\\\frac {1}{k}}+B_{2}\\\\left({\\\\frac {k}{n}}\\\\right)^{4/d}\\\\right\\\\}\\\\{1+o(1)\\\\},}',\n",
       "   'for some constants',\n",
       "   'B',\n",
       "   '1',\n",
       "   '{\\\\displaystyle B_{1}}',\n",
       "   'and',\n",
       "   'B',\n",
       "   '2',\n",
       "   '{\\\\displaystyle B_{2}}',\n",
       "   '.',\n",
       "   'The choice',\n",
       "   'k',\n",
       "   '∗',\n",
       "   '=',\n",
       "   '⌊',\n",
       "   'B',\n",
       "   'n',\n",
       "   '4',\n",
       "   'd',\n",
       "   '+',\n",
       "   '4',\n",
       "   '⌋',\n",
       "   '{\\\\displaystyle k^{*}=\\\\lfloor Bn^{\\\\frac {4}{d+4}}\\\\rfloor }',\n",
       "   'offers a trade off between the two terms in the above display, for which the',\n",
       "   'k',\n",
       "   '∗',\n",
       "   '{\\\\displaystyle k^{*}}',\n",
       "   '-nearest neighbour error converges to the Bayes error at the optimal (minimax) rate',\n",
       "   'O',\n",
       "   '(',\n",
       "   'n',\n",
       "   '−',\n",
       "   '4',\n",
       "   'd',\n",
       "   '+',\n",
       "   '4',\n",
       "   ')',\n",
       "   '{\\\\displaystyle {\\\\mathcal {O}}(n^{-{\\\\frac {4}{d+4}}})}',\n",
       "   '.',\n",
       "   'Metric learning',\n",
       "   'The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning. Popular algorithms are neighbourhood components analysis and large margin nearest neighbor. Supervised metric learning algorithms use the label information to learn a new metric or pseudo-metric.',\n",
       "   'Feature extraction',\n",
       "   'When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called feature extraction. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying',\n",
       "   'k',\n",
       "   '-NN algorithm on the transformed data in feature space.',\n",
       "   'An example of a typical computer vision computation pipeline for face recognition using',\n",
       "   'k',\n",
       "   '-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with OpenCV):',\n",
       "   'Haar face detection',\n",
       "   'Mean-shift tracking analysis',\n",
       "   'PCA or Fisher LDA projection into feature space, followed by',\n",
       "   'k',\n",
       "   '-NN classification',\n",
       "   'Dimension reduction',\n",
       "   'For high-dimensional data (e.g., with number of dimensions more than 10) dimension reduction is usually performed prior to applying the',\n",
       "   'k',\n",
       "   '-NN algorithm in order to avoid the effects of the curse of dimensionality.',\n",
       "   'The curse of dimensionality in the',\n",
       "   'k',\n",
       "   '-NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same).',\n",
       "   'Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by',\n",
       "   'k',\n",
       "   '-NN on feature vectors in reduced-dimension space. In machine learning this process is also called low-dimensional embedding.',\n",
       "   'For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional time series) running a fast',\n",
       "   'approximate',\n",
       "   'k',\n",
       "   '-NN search using locality sensitive hashing, \"random projections\", \"sketches\"  or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option.',\n",
       "   'Decision boundary',\n",
       "   'Nearest neighbor rules in effect implicitly compute the decision boundary. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity.',\n",
       "   'Data reduction',\n",
       "   'Data reduction is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification. Those data are called the',\n",
       "   'prototypes',\n",
       "   'and can be found as follows:',\n",
       "   'Select the',\n",
       "   'class-outliers',\n",
       "   ', that is, training data that are classified incorrectly by',\n",
       "   'k',\n",
       "   '-NN (for a given',\n",
       "   'k',\n",
       "   ')',\n",
       "   'Separate the rest of the data into two sets: (i) the prototypes that are used for the classification decisions and (ii) the',\n",
       "   'absorbed points',\n",
       "   'that can be correctly classified by',\n",
       "   'k',\n",
       "   '-NN using prototypes. The absorbed points can then be removed from the training set.',\n",
       "   'Selection of class-outliers',\n",
       "   'A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include:',\n",
       "   'random error',\n",
       "   'insufficient training examples of this class (an isolated example appears instead of a cluster)',\n",
       "   'missing important features (the classes are separated in other dimensions which we do not know)',\n",
       "   'too many training examples of other classes (unbalanced classes) that create a \"hostile\" background for the given small class',\n",
       "   'Class outliers with',\n",
       "   'k',\n",
       "   '-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers,',\n",
       "   'k>r>0',\n",
       "   ', a training example is called a',\n",
       "   '(k,r)',\n",
       "   'NN class-outlier if its',\n",
       "   'k',\n",
       "   'nearest neighbors include more than',\n",
       "   'r',\n",
       "   'examples of other classes.',\n",
       "   'CNN for data reduction',\n",
       "   'Condensed nearest neighbor (CNN, the',\n",
       "   'Hart algorithm',\n",
       "   ') is an algorithm designed to reduce the data set for',\n",
       "   'k',\n",
       "   '-NN classification. It selects the set of prototypes',\n",
       "   'U',\n",
       "   'from the training data, such that 1NN with',\n",
       "   'U',\n",
       "   'can classify the examples almost as accurately as 1NN does with the whole data set.',\n",
       "   'Given a training set',\n",
       "   'X',\n",
       "   ', CNN works iteratively:',\n",
       "   'Scan all elements of',\n",
       "   'X',\n",
       "   ', looking for an element',\n",
       "   'x',\n",
       "   'whose nearest prototype from',\n",
       "   'U',\n",
       "   'has a different label than',\n",
       "   'x',\n",
       "   '.',\n",
       "   'Remove',\n",
       "   'x',\n",
       "   'from',\n",
       "   'X',\n",
       "   'and add it to',\n",
       "   'U',\n",
       "   'Repeat the scan until no more prototypes are added to',\n",
       "   'U',\n",
       "   '.',\n",
       "   'Use',\n",
       "   'U',\n",
       "   'instead of',\n",
       "   'X',\n",
       "   'for classification. The examples that are not prototypes are called \"absorbed\" points.',\n",
       "   'It is efficient to scan the training examples in order of decreasing border ratio. The border ratio of a training example',\n",
       "   'x',\n",
       "   'is defined as',\n",
       "   'a',\n",
       "   '(',\n",
       "   'x',\n",
       "   ') =',\n",
       "   '||',\n",
       "   \"x'-y\",\n",
       "   '||',\n",
       "   '/',\n",
       "   '||',\n",
       "   'x-y',\n",
       "   '||',\n",
       "   'where',\n",
       "   '||',\n",
       "   'x-y',\n",
       "   '||',\n",
       "   'is the distance to the closest example',\n",
       "   'y',\n",
       "   'having a different color than',\n",
       "   'x',\n",
       "   ', and',\n",
       "   '||',\n",
       "   \"x'-y\",\n",
       "   '||',\n",
       "   'is the distance from',\n",
       "   'y',\n",
       "   'to its closest example',\n",
       "   \"x'\",\n",
       "   'with the same label as',\n",
       "   'x',\n",
       "   '.',\n",
       "   'The border ratio is in the interval [0,1] because',\n",
       "   '||',\n",
       "   \"x'-y\",\n",
       "   '||',\n",
       "   'never exceeds',\n",
       "   '||',\n",
       "   'x-y',\n",
       "   '||',\n",
       "   '. This ordering gives preference to the borders of the classes for inclusion in the set of prototypes',\n",
       "   'U',\n",
       "   '. A point of a different label than',\n",
       "   'x',\n",
       "   'is called external to',\n",
       "   'x',\n",
       "   '. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is',\n",
       "   'x',\n",
       "   'and its label is red. External points are blue and green. The closest to',\n",
       "   'x',\n",
       "   'external point is',\n",
       "   'y',\n",
       "   '. The closest to',\n",
       "   'y',\n",
       "   'red point is',\n",
       "   \"x'\",\n",
       "   '. The border ratio',\n",
       "   'a',\n",
       "   '(',\n",
       "   'x',\n",
       "   ') = ||',\n",
       "   \"x'-y\",\n",
       "   '|| / ||',\n",
       "   'x-y',\n",
       "   '||',\n",
       "   'is the attribute of the initial point',\n",
       "   'x',\n",
       "   '.',\n",
       "   'Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors). Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet.',\n",
       "   'CNN model reduction for k-NN classifiers',\n",
       "   'k',\n",
       "   '-NN regression',\n",
       "   'In',\n",
       "   'k',\n",
       "   '-NN regression, the',\n",
       "   'k',\n",
       "   '-NN algorithm is used for estimating continuous variables. One such algorithm uses a weighted average of the',\n",
       "   'k',\n",
       "   'nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows:',\n",
       "   'Compute the Euclidean or Mahalanobis distance from the query example to the labeled examples.',\n",
       "   'Order the labeled examples by increasing distance.',\n",
       "   'Find a heuristically optimal number',\n",
       "   'k',\n",
       "   'of nearest neighbors, based on RMSE. This is done using cross validation.',\n",
       "   'Calculate an inverse distance weighted average with the',\n",
       "   'k',\n",
       "   '-nearest multivariate neighbors.',\n",
       "   'k',\n",
       "   '-NN outlier',\n",
       "   'The distance to the',\n",
       "   'k',\n",
       "   'th nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in anomaly detection. The larger the distance to the',\n",
       "   'k',\n",
       "   '-NN, the lower the local density, the more likely the query point is an outlier. Although quite simple, this outlier model, along with another classic data mining method, local outlier factor, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis.',\n",
       "   'Validation of results',\n",
       "   'A confusion matrix or \"matching matrix\" is often used as a tool to validate the accuracy of',\n",
       "   'k',\n",
       "   '-NN classification. More robust statistical methods such as likelihood-ratio test can also be applied.',\n",
       "   'See also',\n",
       "   'Nearest centroid classifier',\n",
       "   'Closest pair of points problem',\n",
       "   'References',\n",
       "   'Further reading',\n",
       "   'When Is \"Nearest Neighbor\" Meaningful?',\n",
       "   'Belur V. Dasarathy, ed. (1991).',\n",
       "   'Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques',\n",
       "   '. ISBN\\xa00-8186-8930-7.',\n",
       "   'Shakhnarovish, Darrell, and Indyk, eds. (2005).',\n",
       "   'Nearest-Neighbor Methods in Learning and Vision',\n",
       "   '. MIT Press. ISBN\\xa00-262-19547-X.',\n",
       "   'CS1 maint: Uses editors parameter (link)',\n",
       "   'Mäkelä H Pekkarinen A (2004-07-26). \"Estimation of forest stand volumes by Landsat TM imagery and stand-level field-inventory data\".',\n",
       "   'Forest Ecology and Management',\n",
       "   '.',\n",
       "   '196',\n",
       "   '(2–3): 245–255. doi:10.1016/j.foreco.2004.02.049.',\n",
       "   'Fast k nearest neighbor search using GPU. In Proceedings of the CVPR Workshop on Computer Vision on GPU, Anchorage, Alaska, USA, June 2008. V. Garcia and E. Debreuve and M. Barlaud.',\n",
       "   'Scholarpedia article on',\n",
       "   'k',\n",
       "   '-NN',\n",
       "   'google-all-pairs-similarity-search'],\n",
       "  '_id': ObjectId('5a21fd9e1e6b1f001c45d470')},\n",
       " {'Gene prediction': ['In computational biology,',\n",
       "   'gene prediction',\n",
       "   'or',\n",
       "   'gene finding',\n",
       "   'refers to the process of identifying the regions of genomic DNA that encode genes. This includes protein-coding genes as well as RNA genes, but may also include prediction of other functional elements such as regulatory regions. Gene finding is one of the first and most important steps in understanding the genome of a species once it has been sequenced.',\n",
       "   'In its earliest days, \"gene finding\" was based on painstaking experimentation on living cells and organisms. Statistical analysis of the rates of homologous recombination of several different genes could determine their order on a certain chromosome, and information from many such experiments could be combined to create a genetic map specifying the rough location of known genes relative to each other. Today, with comprehensive genome sequence and powerful computational resources at the disposal of the research community, gene finding has been redefined as a largely computational problem.',\n",
       "   'Determining that a sequence is functional should be distinguished from determining the function of the gene or its product. Predicting the function of a gene and confirming that the gene prediction is accurate still demands',\n",
       "   'in vivo',\n",
       "   'experimentation through gene knockout and other assays, although frontiers of bioinformatics research are making it increasingly possible to predict the function of a gene based on its sequence alone.',\n",
       "   'Gene prediction is one of the key steps in genome annotation, following sequence assembly, the filtering of non-coding regions and repeat masking.',\n",
       "   \"Gene prediction is closely related to the so-called 'target search problem' investigating how DNA-binding proteins (transcription factors) locate specific binding sites within the genome. Many aspects of structural gene prediction are based on current understanding of underlying biochemical processes in the cell such as gene transcription, translation, protein–protein interactions and regulation processes, which are subject of active research in the various omics fields such as transcriptomics, proteomics, metabolomics, and more generally structural and functional genomics.\",\n",
       "   'Empirical methods',\n",
       "   'In empirical (similarity, homology or evidence-based) gene finding systems, the target genome is searched for sequences that are similar to extrinsic evidence in the form of the known expressed sequence tags, messenger RNA (mRNA), protein products, and homologous or orthologous sequences. Given an mRNA sequence, it is trivial to derive a unique genomic DNA sequence from which it had to have been transcribed. Given a protein sequence, a family of possible coding DNA sequences can be derived by reverse translation of the genetic code. Once candidate DNA sequences have been determined, it is a relatively straightforward algorithmic problem to efficiently search a target genome for matches, complete or partial, and exact or inexact. Given a sequence, local alignment algorithms such as BLAST, FASTA and Smith-Waterman look for regions of similarity between the target sequence and possible candidate matches. Matches can be complete or partial, and exact or inexact. The success of this approach is limited by the contents and accuracy of the sequence database.',\n",
       "   \"A high degree of similarity to a known messenger RNA or protein product is strong evidence that a region of a target genome is a protein-coding gene. However, to apply this approach systemically requires extensive sequencing of mRNA and protein products. Not only is this expensive, but in complex organisms, only a subset of all genes in the organism's genome are expressed at any given time, meaning that extrinsic evidence for many genes is not readily accessible in any single cell culture. Thus, to collect extrinsic evidence for most or all of the genes in a complex organism requires the study of many hundreds or thousands of cell types, which presents further difficulties. For example, some human genes may be expressed only during development as an embryo or fetus, which might be difficult to study for ethical reasons.\",\n",
       "   'Despite these difficulties, extensive transcript and protein sequence databases have been generated for human as well as other important model organisms in biology, such as mice and yeast. For example, the RefSeq database contains transcript and protein sequence from many different species, and the Ensembl system comprehensively maps this evidence to human and several other genomes. It is, however, likely that these databases are both incomplete and contain small but significant amounts of erroneous data.',\n",
       "   'New high-throughput transcriptome sequencing technologies such as RNA-Seq and ChIP-sequencing open opportunities for incorporating additional extrinsic evidence into gene prediction and validation, and allow structurally rich and more accurate alternative to previous methods of measuring gene expression such as expressed sequence tag or DNA microarray.',\n",
       "   'Major challenges involved in gene prediction involve dealing with sequencing errors in raw DNA data, dependence on the quality of the sequence assembly, handling short reads, frameshift mutations, overlapping genes and incomplete genes.',\n",
       "   \"In prokaryotes it's essential to consider horizontal gene transfer when searching for gene sequence homology. An additional important factor underused in current gene detection tools is existence of gene clusters—operons in both prokaryotes and eukaryotes. Most popular gene detectors treat each gene in isolation, independent of others, which is not biologically accurate.\",\n",
       "   'Ab initio',\n",
       "   'methods',\n",
       "   'Ab Initio gene prediction is an intrinsic method based on gene content and signal detection. Because of the inherent expense and difficulty in obtaining extrinsic evidence for many genes, it is also necessary to resort to',\n",
       "   'ab initio',\n",
       "   'gene finding, in which the genomic DNA sequence alone is systematically searched for certain tell-tale signs of protein-coding genes. These signs can be broadly categorized as either',\n",
       "   'signals',\n",
       "   ', specific sequences that indicate the presence of a gene nearby, or',\n",
       "   'content',\n",
       "   ', statistical properties of the protein-coding sequence itself.',\n",
       "   'Ab initio',\n",
       "   'gene finding might be more accurately characterized as gene',\n",
       "   'prediction',\n",
       "   ', since extrinsic evidence is generally required to conclusively establish that a putative gene is functional.',\n",
       "   'In the genomes of prokaryotes, genes have specific and relatively well-understood promoter sequences (signals), such as the Pribnow box and transcription factor binding sites, which are easy to systematically identify. Also, the sequence coding for a protein occurs as one contiguous open reading frame (ORF), which is typically many hundred or thousands of base pairs long. The statistics of stop codons are such that even finding an open reading frame of this length is a fairly informative sign. (Since 3 of the 64 possible codons in the genetic code are stop codons, one would expect a stop codon approximately every 20–25 codons, or 60–75 base pairs, in a random sequence.) Furthermore, protein-coding DNA has certain periodicities and other statistical properties that are easy to detect in sequence of this length. These characteristics make prokaryotic gene finding relatively straightforward, and well-designed systems are able to achieve high levels of accuracy.',\n",
       "   'Ab initio',\n",
       "   'gene finding in eukaryotes, especially complex organisms like humans, is considerably more challenging for several reasons. First, the promoter and other regulatory signals in these genomes are more complex and less well-understood than in prokaryotes, making them more difficult to reliably recognize. Two classic examples of signals identified by eukaryotic gene finders are CpG islands and binding sites for a poly(A) tail.',\n",
       "   'Second, splicing mechanisms employed by eukaryotic cells mean that a particular protein-coding sequence in the genome is divided into several parts (exons), separated by non-coding sequences (introns). (Splice sites are themselves another signal that eukaryotic gene finders are often designed to identify.) A typical protein-coding gene in humans might be divided into a dozen exons, each less than two hundred base pairs in length, and some as short as twenty to thirty. It is therefore much more difficult to detect periodicities and other known content properties of protein-coding DNA in eukaryotes.',\n",
       "   'Advanced gene finders for both prokaryotic and eukaryotic genomes typically use complex probabilistic models, such as hidden Markov models (HMMs) to combine information from a variety of different signal and content measurements. The GLIMMER system is a widely used and highly accurate gene finder for prokaryotes. GeneMark is another popular approach. Eukaryotic',\n",
       "   'ab initio',\n",
       "   'gene finders, by comparison, have achieved only limited success; notable examples are the GENSCAN and geneid programs. The SNAP gene finder is HMM-based like Genscan, and attempts to be more adaptable to different organisms, addressing problems related to using a gene finder on a genome sequence that it was not trained against. A few recent approaches like mSplicer, CONTRAST, or mGene also use machine learning techniques like support vector machines for successful gene prediction. They build a discriminative model using hidden Markov support vector machines or conditional random fields to learn an accurate gene prediction scoring function.',\n",
       "   'Ab Initio',\n",
       "   'methods have been benchmarked, with some approaching 100% sensitivity, however as the sensitivity increases, accuracy suffers as a result of increased false positives.',\n",
       "   'Other signals',\n",
       "   'Among the derived signals used for prediction are statistics resulting from the sub-sequence statistics like k-mer statistics, Isochore (genetics) or Compositional domain GC composition/uniformity/entropy, sequence and frame length, Intron/Exon/Donor/Acceptor/Promoter and Ribosomal binding site vocabulary, Fractal dimension, Fourier transform of a pseudo-number-coded DNA, Z-curve parameters and certain run features.',\n",
       "   'It has been suggested that signals other than those directly detectable in sequences may improve gene prediction. For example, the role of secondary structure in the identification of regulatory motifs has been reported. In addition, it has been suggested that RNA secondary structure prediction helps splice site prediction.',\n",
       "   'Neural networks',\n",
       "   'Neural networks are computational models that excel at machine learning and pattern recognition. Neural networks must be trained with example data before being able to generalise for experimental data, and tested against benchmark data. Neural networks are able to come up with approximate solutions to problems that are hard to solve algorithmically, provided there is sufficient training data. When applied to gene prediction, neural networks can be used alongside other',\n",
       "   'ab initio',\n",
       "   'methods to predict or identify biological features such as splice sites. One approach involves using a sliding window, which traverses the sequence data in an overlapping manner. The output at each position is a score based on whether the network thinks the window contains a donor splice site or an acceptor splice site. Larger windows offer more accuracy but also require more computational power. A neural network is an example of a signal sensor as its goal is to identify a functional site in the genome.',\n",
       "   'Combined approaches',\n",
       "   'Programs such as Maker combine extrinsic and',\n",
       "   'ab initio',\n",
       "   'approaches by mapping protein and EST data to the genome to validate',\n",
       "   'ab initio',\n",
       "   'predictions. Augustus, which may be used as part of the Maker pipeline, can also incorporate hints in the form of EST alignments or protein profiles to increase the accuracy of the gene prediction.',\n",
       "   'Comparative genomics approaches',\n",
       "   'As the entire genomes of many different species are sequenced, a promising direction in current research on gene finding is a comparative genomics approach.',\n",
       "   'This is based on the principle that the forces of natural selection cause genes and other functional elements to undergo mutation at a slower rate than the rest of the genome, since mutations in functional elements are more likely to negatively impact the organism than mutations elsewhere. Genes can thus be detected by comparing the genomes of related species to detect this evolutionary pressure for conservation. This approach was first applied to the mouse and human genomes, using programs such as SLAM, SGP and TWINSCAN/N-SCAN and CONTRAST.',\n",
       "   'Multiple informants',\n",
       "   'TWINSCAN examined only human-mouse synteny to look for orthologous genes. Programs such as N-SCAN and CONTRAST allowed the incorporation of alignments from multiple organisms, or in the case of N-SCAN, a single alternate organism from the target. The use of multiple informants can lead to significant improvements in accuracy.',\n",
       "   'CONTRAST is composed of two elements. The first is a smaller classifier, identifying donor splice sites and acceptor splice sites as well as start and stop codons. The second element involves constructing a full model using machine learning. Breaking the problem into two means that smaller targeted data sets can be used to train the classifiers, and that classifier can operate independently and be trained with smaller windows. The full model can use the independent classifier, and not have to waste computational time or model complexity re-classifying intron-exon boundaries. The paper in which CONTRAST is introduced proposes that their method (and those of TWINSCAN, etc.) be classified as',\n",
       "   'de novo',\n",
       "   'gene assembly, using alternate genomes, and identifying it as distinct from',\n",
       "   'ab initio',\n",
       "   \", which uses a target 'informant' genomes.\",\n",
       "   'Comparative gene finding can also be used to project high quality annotations from one genome to another. Notable examples include Projector, GeneWise and GeneMapper. Such techniques now play a central role in the annotation of all genomes.',\n",
       "   'Pseudogene prediction',\n",
       "   'Pseudogenes are close relatives of genes, sharing very high sequence homology, but being unable to code for the same protein product. Whilst once relegated as byproducts of gene sequencing, increasingly, as regulatory roles are being uncovered, they are becoming predictive targets in their own right. Pseudogene prediction utilises existing sequence similarity and ab initio methods, whilst adding additional filtering and methods of identifying pseudogene characteristics.',\n",
       "   'Sequence similarity methods can be customised for pseudogene prediction using additional filtering to find candidate pseudogenes. This could use disablement detection, which looks for nonsense or frameshift mutations that would truncate or collapse an otherwise functional coding sequence. Additionally, translating DNA into proteins sequences can be more effective than just straight DNA homology.',\n",
       "   'Content sensors can be filtered according to the differences in statistical properties between pseudogenes and genes, such as a reduced count of CpG islands in pseudogenes, or the differences in G-C content between pseudogenes and their neighbours. Signal sensors also can be honed to pseudogenes, looking for the absence of introns or polyadenine tails.',\n",
       "   'Metagenomic gene prediction',\n",
       "   'Metagenomics is the study of genetic material recovered from the environment, resulting in sequence information from a pool of organisms. Predicting genes is useful for comparative metagenomics.',\n",
       "   'Metagenomics tools also fall into the basic categories of using either sequence similarity approaches (MEGAN4) and ab initio techniques (GLIMMER-MG).',\n",
       "   'Glimmer-MG is an extension to GLIMMER that relies mostly on an ab initio approach for gene finding and by using training sets from related organisms. The prediction strategy is augmented by classification and clustering gene data sets prior to applying ab initio gene prediction methods. The data is clustered by species. This classification method leverages techniques from metagenomic phylogenetic classification. An example of software for this purpose is, Phymm, which uses interpolated markov models—and PhymmBL, which integrates BLAST into the classification routines.',\n",
       "   'MEGAN4 uses a sequence similarity approach, using local alignment against databases of known sequences, but also attempts to classify using additional information on functional roles, biological pathways and enzymes. As in single organism gene prediction, sequence similarity approaches are limited by the size of the database.',\n",
       "   'FragGeneScan and MetaGeneAnnotator are popular gene prediction programs based on Hidden Markov model. These predictors account for sequencing errors, partial genes and work for short reads.',\n",
       "   'Another fast and accurate tool for gene prediction in metagenomes is MetaGeneMark. This tool is used by the DOE Joint Genome Institute to annotate IMG/M, the largest metagenome collection to date.',\n",
       "   'See also',\n",
       "   'List of gene prediction software',\n",
       "   'Sequence mining',\n",
       "   'Protein function prediction',\n",
       "   'Phylogenetic footprinting',\n",
       "   'Sequence similarity (homology)',\n",
       "   'External links',\n",
       "   'Augustus',\n",
       "   'FGENESH',\n",
       "   'geneid, SGP2',\n",
       "   'Glimmer, GlimmerHMM',\n",
       "   'GenomeThreader',\n",
       "   'ChemGenome',\n",
       "   'GeneMark',\n",
       "   'Gismo',\n",
       "   'mGene',\n",
       "   'StarORF — A multi-platform and web tool for predicting ORFs and obtaining reverse complement sequence',\n",
       "   'Maker - A portable and easily configurable genome annotation pipeline',\n",
       "   'References'],\n",
       "  '_id': ObjectId('5a21fd9e1e6b1f001c45d471')},\n",
       " {'Transiogram': ['Transiogram',\n",
       "   'is the accompanying spatial correlation measure of simplified Markov chain random field (MCRF) models based on the conditional independence assumption and an important part of Markov chain geostatistics. It is defined as a transition probability function over the distance lag. Simply, a transiogram refers to a transition probability-lag diagram. Transiograms include auto-transiograms and cross-transiograms. The former represent the spatial auto-correlation of a single category, and the latter represent the spatial interclass relationships among different categories. Experimental transiograms can be directly estimated from sparse sample data. Transiogram models, which provide transition probabilities at any lags for Markov chain modeling, can be further acquired through model fitting of experimental transiograms. In general, the transiogram is a spatial correlation measure following the style of variogram, and it includes a set of concepts and a set of methods for obtaining transition probability values from sample data and provide transition probabilities values for simplified MCRF models.',\n",
       "   'References',\n",
       "   'Li, W. (2007) \"Transiograms for characterizing spatial variability of soil classes\".',\n",
       "   'Soil Science Society of America Journal',\n",
       "   '71(3): 881-893',\n",
       "   'Li W, Zhang C (2010) \"Linear interpolation and joint model fitting of experimental transiograms for Markov chain simulation of categorical spatial variables\".',\n",
       "   'Int J Geogr Info Sci',\n",
       "   '24: 821–839.',\n",
       "   'http://gis.geog.uconn.edu/weidong/MCG/Transiogram.htm'],\n",
       "  '_id': ObjectId('5a21fd9e1e6b1f001c45d472')}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show first five items of the dictionary\n",
    "ml_dict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1106, 1106)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counts for Machine learing pages storage for contents by sentence, content text as a whole \n",
    "db_wiki_ref.count(), db_wiki_whole_ref.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Data Frame with content and its page title for all Machine Learning Categories & Sub categories\n",
    "for x in list(ml_page_list):\n",
    "    temp_df = get_content_df(x)\n",
    "    ml_content_df = ml_content_df.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle storage for data frame \n",
    "ml_content_df.to_pickle(\"ml_content_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_content_df = pd.read_pickle(\"ml_content_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numberic label for title\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "ml_content_df['title_num'] = le.fit_transform(ml_content_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>title_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9292749</td>\n",
       "      <td>Forward–backward_algorithm</td>\n",
       "      <td>Theforward–backward algorithmis an inference a...</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47012074</td>\n",
       "      <td>Neural_Designer</td>\n",
       "      <td>Neural Designeris a software tool for data ana...</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22562715</td>\n",
       "      <td>Clustering_high-dimensional_data</td>\n",
       "      <td>Clustering high-dimensional datais the cluster...</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_id                             title  \\\n",
       "0   9292749        Forward–backward_algorithm   \n",
       "0  47012074                   Neural_Designer   \n",
       "0  22562715  Clustering_high-dimensional_data   \n",
       "\n",
       "                                             content  title_num  \n",
       "0  Theforward–backward algorithmis an inference a...        365  \n",
       "0  Neural Designeris a software tool for data ana...        726  \n",
       "0  Clustering high-dimensional datais the cluster...        151  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_content_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare TFIDF Term Frequency * inverse Document Frequency\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1, stop_words = 'english')\n",
    "\n",
    "ml_tfidf_term_matrix_sps = tfidf_vectorizer.fit_transform(ml_content_df.content)\n",
    "\n",
    "ml_tfidf_term_matrix_df = pd.DataFrame(ml_tfidf_term_matrix_sps.toarray(),\n",
    "                                       index=ml_content_df.content,\n",
    "                                       columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     AlphaGo versus Lee Sedol, orGoogle DeepMind Ch...\n",
       "1      AlphaGo won all but the fourth game; all game...\n",
       "2      The match has been compared with the historic...\n",
       "3     The winner of the match was slated to win $1 m...\n",
       "4      Since AlphaGo won, Google DeepMind stated tha...\n",
       "5      Lee received $170,000 ($150,000 for participa...\n",
       "6     After the match, The Korea Baduk Association a...\n",
       "7      It was given in recognition of AlphaGo\\'s \"si...\n",
       "8      This match was chosen byScienceas one of the ...\n",
       "9     BackgroundDifficult challenge in artificial in...\n",
       "10     It has long been considered a difficult chall...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Search Material: First 11 Sentences of the First Paragraph of \"AlphaGo versus Lee Sedol\" Wiki Page\n",
    "\n",
    "alpha_go = get_content_df('AlphaGo versus Lee Sedol')\n",
    "\n",
    "alpha_go_str = str(alpha_go['content'].values).split('.')[:11]\n",
    "\n",
    "alpha_go_df = pd.DataFrame(data = [x for x in alpha_go_str])\n",
    "\n",
    "alpha_go_df[0][0]='AlphaGo versus Lee Sedol, orGoogle DeepMind Challenge Match, was a five-game Go match between 18-time world champion Lee Sedol and AlphaGo, a computer Go program developed by Google DeepMind, played in Seoul, South Korea between 9 and 15 March 2016'\n",
    "\n",
    "alpha_go_df.columns=['content']\n",
    "\n",
    "alpha_go_df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create TFIDF for search sentences \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1, stop_words = 'english')\n",
    "\n",
    "ag_tfidf_term_matrix_sps = tfidf_vectorizer.fit_transform(alpha_go_df.content)\n",
    "\n",
    "ag_tfidf_term_matrix_df = pd.DataFrame(ag_tfidf_term_matrix_sps.toarray(),\n",
    "                                       index=alpha_go_df.content,\n",
    "                                       columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>15</th>\n",
       "      <th>150</th>\n",
       "      <th>170</th>\n",
       "      <th>18</th>\n",
       "      <th>1997</th>\n",
       "      <th>20</th>\n",
       "      <th>2016</th>\n",
       "      <th>22</th>\n",
       "      <th>additional</th>\n",
       "      <th>...</th>\n",
       "      <th>thinking</th>\n",
       "      <th>time</th>\n",
       "      <th>unicef</th>\n",
       "      <th>versus</th>\n",
       "      <th>win</th>\n",
       "      <th>winner</th>\n",
       "      <th>winning</th>\n",
       "      <th>won</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The winner of the match was slated to win $1 million</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478484</td>\n",
       "      <td>0.478484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    000   15  150  170   18  \\\n",
       "content                                                                       \n",
       "The winner of the match was slated to win $1 mi...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                                                    1997   20  2016   22  \\\n",
       "content                                                                    \n",
       "The winner of the match was slated to win $1 mi...   0.0  0.0   0.0  0.0   \n",
       "\n",
       "                                                    additional  ...   \\\n",
       "content                                                         ...    \n",
       "The winner of the match was slated to win $1 mi...         0.0  ...    \n",
       "\n",
       "                                                    thinking  time  unicef  \\\n",
       "content                                                                      \n",
       "The winner of the match was slated to win $1 mi...       0.0   0.0     0.0   \n",
       "\n",
       "                                                    versus       win  \\\n",
       "content                                                                \n",
       "The winner of the match was slated to win $1 mi...     0.0  0.478484   \n",
       "\n",
       "                                                      winner  winning  won  \\\n",
       "content                                                                      \n",
       "The winner of the match was slated to win $1 mi...  0.478484      0.0  0.0   \n",
       "\n",
       "                                                    world  year  \n",
       "content                                                          \n",
       "The winner of the match was slated to win $1 mi...    0.0   0.0  \n",
       "\n",
       "[1 rows x 92 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a random sentence from \"AlphaGo versus Lee Sedol\" page. \n",
    "\n",
    "ag_random_search_df = ag_tfidf_term_matrix_df.sample()\n",
    "ag_random_search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Search sentence to the machine learning TFIDF\n",
    "ml_with_search_term = ml_tfidf_term_matrix_df.append(ag_random_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_with_search_term.fillna(value = 0.0, inplace=True)\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_with_search_term.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute SVD of Augmented Document Term Matrix \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components = 50\n",
    "SVD = TruncatedSVD(n_components)\n",
    "ml_component_names = [\"component_\"+str(i+1) for i in range(n_components)]\n",
    "\n",
    "ml_svd_matrix = SVD.fit_transform(ml_with_search_term)\n",
    "\n",
    "ml_svd_df = pd.DataFrame(ml_svd_matrix, \n",
    "                      index=ml_with_search_term.index, \n",
    "                      columns=ml_component_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "      <th>component_3</th>\n",
       "      <th>component_4</th>\n",
       "      <th>component_5</th>\n",
       "      <th>component_6</th>\n",
       "      <th>component_7</th>\n",
       "      <th>component_8</th>\n",
       "      <th>component_9</th>\n",
       "      <th>component_10</th>\n",
       "      <th>...</th>\n",
       "      <th>component_41</th>\n",
       "      <th>component_42</th>\n",
       "      <th>component_43</th>\n",
       "      <th>component_44</th>\n",
       "      <th>component_45</th>\n",
       "      <th>component_46</th>\n",
       "      <th>component_47</th>\n",
       "      <th>component_48</th>\n",
       "      <th>component_49</th>\n",
       "      <th>component_50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The winner of the match was slated to win $1 million</th>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>-0.011086</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>-0.010181</td>\n",
       "      <td>-0.00992</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049233</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>-0.002035</td>\n",
       "      <td>-0.009223</td>\n",
       "      <td>-0.049225</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.011011</td>\n",
       "      <td>-0.007566</td>\n",
       "      <td>0.020353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    component_1  component_2  \\\n",
       "content                                                                        \n",
       "The winner of the match was slated to win $1 mi...     0.006572     0.014576   \n",
       "\n",
       "                                                    component_3  component_4  \\\n",
       "content                                                                        \n",
       "The winner of the match was slated to win $1 mi...    -0.008371    -0.000466   \n",
       "\n",
       "                                                    component_5  component_6  \\\n",
       "content                                                                        \n",
       "The winner of the match was slated to win $1 mi...      -0.0101    -0.011086   \n",
       "\n",
       "                                                    component_7  component_8  \\\n",
       "content                                                                        \n",
       "The winner of the match was slated to win $1 mi...     0.005374    -0.010181   \n",
       "\n",
       "                                                    component_9  component_10  \\\n",
       "content                                                                         \n",
       "The winner of the match was slated to win $1 mi...     -0.00992      0.001727   \n",
       "\n",
       "                                                        ...       \\\n",
       "content                                                 ...        \n",
       "The winner of the match was slated to win $1 mi...      ...        \n",
       "\n",
       "                                                    component_41  \\\n",
       "content                                                            \n",
       "The winner of the match was slated to win $1 mi...     -0.049233   \n",
       "\n",
       "                                                    component_42  \\\n",
       "content                                                            \n",
       "The winner of the match was slated to win $1 mi...      0.005342   \n",
       "\n",
       "                                                    component_43  \\\n",
       "content                                                            \n",
       "The winner of the match was slated to win $1 mi...     -0.000349   \n",
       "\n",
       "                                                    component_44  \\\n",
       "content                                                            \n",
       "The winner of the match was slated to win $1 mi...     -0.002035   \n",
       "\n",
       "                                                    component_45  \\\n",
       "content                                                            \n",
       "The winner of the match was slated to win $1 mi...     -0.009223   \n",
       "\n",
       "                                                    component_46  \\\n",
       "content                                                            \n",
       "The winner of the match was slated to win $1 mi...     -0.049225   \n",
       "\n",
       "                                                    component_47  \\\n",
       "content                                                            \n",
       "The winner of the match was slated to win $1 mi...      0.000058   \n",
       "\n",
       "                                                    component_48  \\\n",
       "content                                                            \n",
       "The winner of the match was slated to win $1 mi...      0.011011   \n",
       "\n",
       "                                                    component_49  component_50  \n",
       "content                                                                         \n",
       "The winner of the match was slated to win $1 mi...     -0.007566      0.020353  \n",
       "\n",
       "[1 rows x 50 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find our search sentence and indentify its topic ratio \n",
    "ml_search_term_svd_vector = ml_svd_df.loc[ag_random_search_df.index]\n",
    "ml_search_term_svd_vector[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The winner of the match was slated to win $1 million</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MysteryVibeis a British manufacturer of sex toys.HistoryMysteryVibe was founded by a group of researchers, engineers and designers. Inspired by trends in smartphones like Nokia Morph, the founders came up with the idea of creating a sex toy that would adapt to any body shape and vibrate to any pattern. They continued to research for a number of years before formally starting the company in May 2014, when they were incubated by London-based industrial design firm Seymourpowell.MysteryVibe released their iOS app on the Apple App Store in December 2015 and their Android app on Google Play in September 2016. The apps are designed without any adult themes to support MysteryVibe's wider goal of improving sex education for teenagers and are the only apps in their class to be rated 12+. Their apps have been downloaded more than 250,000 times since launch.MysteryVibe's flagship product, Crescendo, is the world's first vibrator that can be bent to adapt to any body shape. Crescendo was the first crowdfunding project to offer its backers 2 versions of their product:PilotandRetail. They ran what they called the #Pilot1000programme for their first 1,000 users to get feedback on their Crescendo product. The #Pilot1000users spanned 48 countries and included both backers and experts. MysteryVibe gave all 1,000 users full access to their founding CEO with direct email, phone and Skype. They then used the feedback they received to make the finalRetailCrescendo.InvestmentDue to the lean model adopted by MysteryVibe withcollectivesandcollaborations, they were able to build the company from a sketch to shipped products with less than £1m ($1.5m) in funding. They were also unique in raising 100% of the money from Angel investors without any recourse to Venture capital. As of Q2 2017, MysteryVibe has raised $3.5m in total funding.AwardsMysteryVibe has won numerous awards for their company, products and founders. Notable mentions are Red Dot, IDA Design, The Drum, and Excellence in Design. Their biggest recognition has been the Young Guns award.Virgin #VOOM2016In June 2016, MysteryVibe became the first pleasure product to be featured by Virgin in their #VOOM competition. They were showcased under the Export Awards category for exporting their products to over 50 countries worldwide. This led to their selection to the Hardware Club and a nomination for the 'Best Hardware Startup' award at The Europas in London.Media attentionMysteryVibe is the first brand in their category to have been featured on BBC. Ranked by European investors as No. 9 of the 100 Slush start-ups inCofounder Magazine, MysteryVibe has been named as one of the \"Top 100 Europe's hottest early-stage Founders\" byPathFounders,Europas. They have been listed at No. 7 in the \"12 days of start-ups: Spectacular businesses set for stardom in 2016\".References</th>\n",
       "      <td>0.810189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Traxis a technology company headquartered in Singapore, with offices throughout APAC, Europe, Middle East, North America and South America. Its computer vision technology is used by FMCG companies such as Coca-Cola and Retailers to collect, measure and analyse what is happening on physical shelves.HistoryFounded in 2010, Trax has over 150 customers in the Retail and FMCG industries, including beverage giant Coca-Cola and brewer Anheuser-Busch InBev. Its service is available in 45 markets and the company's development centre is located in Tel Aviv. Trax closed its first round of funding for US$1.1 million, in June, 2011. They opened their Tel-Aviv office in July, 2012, and closed their second round of funding for US$6.4 million in December, 2012. Their third round of funding for US$15.7 million closed in February, 2014. In December 2014 Trax announced its fourth round of investment of US$15 million.In 2015, Trax opened their first two regional offices, London in January, and Brazil in April. In March 2016, Trax established their LATAM headquarters in Atlanta, Georgia. Trax announced a 5th round of funding for US$40 million on June 8, 2016. Two new regional offices were opened in Shanghai and Mexico City, in June and September 2016 respectively. On February 8, 2017, Trax closed their sixth round of funding for US$19.5 million. On June 30, 2017 Trax announced its most recent funding round of US$64 million lead by global private equity giant Warburg Pincus.Mergers and AcquisitionsOn July 12, 2017, Trax announced that they had acquired Nielsen Store Observation (NSO) assets in the USA from Nielsen Corporation.Software and ServicesTrax reduces the time an employee needs to spend on audits to check inventory, shelf display and product promotions. It is also gathers more extensive data such as product assortment, shelf space, pricing, promotions, shelf location and arrangement of products on display. This market intelligence is valuable to Retail and FMCG manufacturers because they pay large sums for space in supermarkets and stores. For example, in the US companies pay approximately $18 billion for shelf space.TechnologyThe computer vision technology uses Artificial Intelligence, fine-grained image recognition, and machine learning engines to convert store images into shelf insights. Trax is able to recognise products that are similar or identical such as branded drinks or shampoo bottles whilst also being able to differentiate between them based on variety and size. It piloted its machine learning algorithms with initial customers, allowing its algorithm to learn about different products. As the company processes more images, the better it gets at recognising the same products in different shapes and sizes.. To date, Trax has recognized more than 8 billion images, and recognizes approximately 400,000 million new products per month.ReferencesExternal LinksOfficial website</th>\n",
       "      <td>0.801146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qloo(pronounced \"clue\") is a company that uses artificial intelligence (AI). An application programming interface (API) provides cultural correlations. It was founded by Alex Elias and received funding from Leonardo DiCaprio, Barry Sternlicht and Pierre Lagrange.Qloo establishes consumer preference correlations via machine learning across multiple proprietary, customer and open-source data across cultural domains including music, film, television, dining, nightlife, fashion, books and travel. The recommender system uses AI to predict correlations for further applications.HistoryQloo was founded in 2012 by chief executive officer Alex Elias and chief operating officer Jay Alger. Elias was formerly a hedge fund manager with APE Capital. He graduated from the University of Southern California, and then developed his idea at law school at New York University. Alger was formerly the CEO of the digital agency Deepend.Qloo was tested on a private website in April 2012. In 2012, Qloo raised $1.4 million in seed funding from investors including Cedric the Entertainer, Danny Masterson, and venture capital firm Kindler Capital. Qloo had a public beta release in November 2012 after its initial funding.In 2013, the company raised an additional $1.6 million from Cross Creek Pictures founding partner Tommy Thompson, and Samih Toukan and Hussam Khoury, founders of Maktoob, an Internet services company purchased by Yahoo! for $164 million in 2009. On November 14, 2013, a website and an iPhone app were announced. The company later released an Android app, and tablet versions, in mid-2014.In 2016, Qloo secured $4.5 million in venture capital investment. The $4.5 million was split between a number of investors, including Barry Sternlicht, Pierre Lagrange and Leonardo DiCaprio. In July 2017, Qloo raised $6.5 million in funding rounds from AXA Strategic Ventures and Elton John.Following the investment, the founders stated in an interview with Tech Crunch that they would use the investment to expand Qloo's database. They hoped the move would secure larger contracts with corporate clients. At the time, clients already included Fortune 500 companies such as Twitter, PepsiCo and BMW.Services and featuresQloo calls itself a cultural AI platform to provide real-time correlation data across domains of culture and entertainment including: film, music, television, dining, nightlife, fashion, books and travel. Each category contains subcategories.Qloo’s knowledge of a user's taste in one category can be utilized to offer suggestions in other categories. Users then rate the suggestions, providing it with feedback for future suggestions. Qloo has partnerships with companies such as Expedia and iTunes.ReferencesExternal linksOfficial website</th>\n",
       "      <td>0.800099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cleverbotis a chatterbot web application that uses an artificial intelligence (AI) algorithm to have conversations with humans. It was created by British AI scientist Rollo Carpenter. It was preceded by Jabberwacky, a chatbot project that began in 1988 and went online in 1997. In its first decade, Cleverbot held several thousand conversations with Carpenter and his associates. Since launching on the web, the number of conversations held has exceeded 200 million. Besides the web application, Cleverbot is also available as an iOS, Android, and Windows Phone app.OperationUnlike some other chatterbots, Cleverbot's responses are not pre-programmed. Instead, it learns from human input: Humans type into the box below the Cleverbot logo and the system finds all keywords or an exact phrase matching the input. After searching through its saved conversations, it responds to the input by finding how a human responded to that input when it was asked, in part or in full, by Cleverbot.Cleverbot participated in a formal Turing test at the 2011 Techniche festival at the Indian Institute of Technology Guwahati on September 3, 2011. Out of the 334 votes cast, Cleverbot was judged to be 59.3% human, compared to the rating of 63.3% human achieved by human participants. A score of 50.05% or higher is often considered to be a passing grade. The software running for the event had to handle just 1 or 2 simultaneous requests, whereas online Cleverbot is usually talking to around 80,000 people at once.DevelopmentsCleverbot is constantly learning, growing in data size at a rate of 4 to 7 million interactions per second. Updates to the software have been mostly behind the scenes. In 2014, Cleverbot was upgraded to use GPU serving techniques. The program chooses how to respond to users fuzzily, the whole of the conversation being compared to the millions that have taken place before. Cleverbot now uses over 279 million interactions, about 3-4% of the data it has already accumulated. The developers of Cleverbot are attempting to build a new version using machine learning techniques.A significant part of the engine behind Cleverbot and an API for accessing it has been made available to developers in the form of Cleverscript. A service for directly accessing Cleverbot has been made available to developers in the form of Cleverbot.io.An app that uses the Cleverscript engine to play a game of 20 Questions, has been launched under the nameClevernator. Unlike other such games, the player asks the questions and it is the role of the AI to understand, and answer factually. An app that allows owners to create and talk to their own small Cleverbot-like AI has been launched, calledCleverme!for Apple products.In early 2017, a Twitch stream of two Google Home devices modified to talk to each other using Cleverbot.io garnered over 700,000 visitors and over 30,000 peak concurrent viewers.See alsoList of chatterbotsOmegleReferencesExternal linksOfficial websiteCleverscript websiteCleverbot.io websiteLivestream of 2 cleverbots chatting with each other on Twitch.tv</th>\n",
       "      <td>0.775899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prismais a photo-editing application that utilizes a neural network and artificial intelligence to transform the image into an artistic effect.The app was created by Alexey Moiseenkov (Russian:Алексей Моисеенков), Oleg Poyaganov, Ilya Frolov, Andrey Usoltsev and it was launched in June 2016 as a free mobile app. A week after its launch, the app gained popularity and received over 7.5 million downloads and over 1 million active users as of July 2016. It debuted on iOS on Apple App Store during the first week of June and it became the leading app at the App Store in Russia and other neighboring countries. On 19 July 2016, the developer launched a beta version of the app for Android and it closed few hours later by developers after receiving feedback from its users. It was later released publicly on 24 July 2016 on Google Play.In July 2016, the developer announced that the video and virtual reality version of the app is currently under development.On July 7, 2017, Prisma launched a new app called Sticky which turns selfies into stickers for sharing to your social feeds.HistoryThe app was created by the team led by Alexey Moiseenkov who also founded the Prisma labs, based in Moscow. Moiseenkov previously worked at Mail.Ru and later resigned from his job to dedicate his time for the development of the app. He said that the development of the app took only one and a half months and the team did not do anything to promote the app.The algorithm that powers the app is based on the open source programming and algorithms behind DeepArt.FeaturesUsers can upload pictures and select a variety of filters to transform the picture into an artistic effect. At launch, the app offered twenty different filters. Additional filters are added daily. In July 2016, Moiseenkov stated that the app will offer forty filters by the end of the month.The image rendering takes place in Prisma labs's servers and it uses a neural network and artificial intelligence to add the artistic effect. The result is delivered back to the user's phone. Unlike other photo editing apps, Prisma renders the image by going through different layers and recreating the image rather than inserting a layer over the image.In August 2016, the iOS version of the app was updated to edit image offline by utilizing the phone's processor for image rendering.ReceptionDownloadsOne week after its debut on iOS App Store, the app was downloaded over 7.5 million times and received over 1 million active users. It also became the top listed app in Russia and its neighboring countries. In the end of July 2016, it was installed over 12.5 million devices with over 1.5 million active users worldwide. According to App Annie, it was listed in the top 10 apps on the App Store in 77 different countries.On the first day of the Android version release, it received over 1.7 million downloads with 50 million pictures processed by the app.Research and technologyThe research paper behind the Prisma App technology is called \"A Neural Algorithm of Artistic Style\" by Leon Gatys, Alexander Ecker and Matthias Bethge and was presented at the premier machine learning conference: Neural Information Processing Systems (NIPS) in 2015. This technology was developed independently and before Prisma, and both the university and the company have no affiliation with one another.Further recent work developed by Stanford University: Perceptual Losses for Real-Time Style Transfer and Super-Resolution by Justin Johnson, Alexandre Alahi and Li Fei-Fei has also been able to create real-time style transfer through video.The code for the previous papers is available at no charge at GitHub for research purposes. The Prisma App (on the industrial front), and Style Transfer and Super Resolution (on the research front) has been made possible thanks to research and development in human perception, texture analysis, convolutional neural networks.See alsoList of Prisma (app) filtersReferencesExternal linksOfficial website</th>\n",
       "      <td>0.771186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    cosine_sim\n",
       "content                                                       \n",
       "The winner of the match was slated to win $1 mi...    1.000000\n",
       "MysteryVibeis a British manufacturer of sex toy...    0.810189\n",
       "Traxis a technology company headquartered in Si...    0.801146\n",
       "Qloo(pronounced \"clue\") is a company that uses ...    0.800099\n",
       "Cleverbotis a chatterbot web application that u...    0.775899\n",
       "Prismais a photo-editing application that utili...    0.771186"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "ml_svd_df['cosine_sim'] = cosine_similarity(ml_svd_df, ml_search_term_svd_vector)\n",
    "\n",
    "ml_svd_df[['cosine_sim']].sort_values('cosine_sim', ascending=False).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the randomly selected sentence didn't contain any info on Alpha Go, the search came out with:\n",
    "1. Mystery Vibels, a British sex toy manufacturer\n",
    "2. Traxis, Vision Tech firm hq'd in Singapore\n",
    "3. Qloo, Leaonardo DiCaprio funded Artifical Intelligence company\n",
    "4. Cleverbotis, Chatterbot Web Application\n",
    "5. Prismals, Photo-editing Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's repeat the same steps for Business software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wiki collection process for business software\n",
    "\n",
    "bs_page_list = set(page_list(\"Category:Business software\"))\n",
    "\n",
    "bs_content_list=[]\n",
    "for title in bs_page_list:\n",
    "    bs_content_list.append(clean_content(title))\n",
    "\n",
    "bs_page_list_2 = [x.replace('.',' ') for x in bs_page_list]\n",
    "\n",
    "bs_new_list = []\n",
    "for i in range(len(bs_page_list_2)):\n",
    "    bs_new_dict = {bs_page_list_2[i]:bs_content_list[i]}\n",
    "    bs_new_list.append(bs_new_dict)\n",
    "\n",
    "for i in bs_new_list:\n",
    "    db_wiki_bs_ref.insert_one(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_dict = list(db_wiki_bs_ref.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4583, 4584)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count for Business software pages \n",
    "db_wiki_bs_ref.count(), db_wiki_bs_whole_ref.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_page_list = set(page_list(\"Category:Business software\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in list(bs_page_list):\n",
    "    temp_df = get_content_df(x)\n",
    "    bs_content_df = bs_content_df.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_content_df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_content_df.to_pickle(\"bs_content_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_content_df = pd.read_pickle(\"bs_content_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21670195</td>\n",
       "      <td>Java_Persistence_Query_Language</td>\n",
       "      <td>TheJava Persistence Query Language(JPQL) is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>469578</td>\n",
       "      <td>Decision_support_system</td>\n",
       "      <td>Adecision support system(DSS) is a system base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32039577</td>\n",
       "      <td>LibreOffice_Writer</td>\n",
       "      <td>LibreOffice Writeris the free and open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24902683</td>\n",
       "      <td>Office_Open_XML_file_formats</td>\n",
       "      <td>TheOffice Open XML file formatsare a set of fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4325491</td>\n",
       "      <td>Bing_(search_engine)</td>\n",
       "      <td>Bingis a web search engine owned and operated ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_id                            title  \\\n",
       "0  21670195  Java_Persistence_Query_Language   \n",
       "0    469578          Decision_support_system   \n",
       "0  32039577               LibreOffice_Writer   \n",
       "0  24902683     Office_Open_XML_file_formats   \n",
       "0   4325491             Bing_(search_engine)   \n",
       "\n",
       "                                             content  \n",
       "0  TheJava Persistence Query Language(JPQL) is a ...  \n",
       "0  Adecision support system(DSS) is a system base...  \n",
       "0  LibreOffice Writeris the free and open-source ...  \n",
       "0  TheOffice Open XML file formatsare a set of fi...  \n",
       "0  Bingis a web search engine owned and operated ...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_content_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a label for target\n",
    "\n",
    "le = LabelEncoder()\n",
    "bs_content_df['title_num'] = le.fit_transform(bs_content_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-36dce8d365d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbs_tfidf_term_matrix_sps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs_tfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs_content_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m bs_tfidf_term_matrix_df = pd.DataFrame(bs_tfidf_term_matrix_sps.toarray(),\n\u001b[0m\u001b[1;32m      8\u001b[0m                                        \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs_content_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                        columns=bs_tfidf_vectorizer.get_feature_names())\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;31m##############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Prepare TFIDF Term Frequency * inverse Document Frequency\n",
    "\n",
    "bs_tfidf_vectorizer = TfidfVectorizer(min_df = 1, stop_words = 'english')\n",
    "\n",
    "bs_tfidf_term_matrix_sps = bs_tfidf_vectorizer.fit_transform(bs_content_df.content)\n",
    "\n",
    "bs_tfidf_term_matrix_df = pd.DataFrame(bs_tfidf_term_matrix_sps.toarray(),\n",
    "                                       index=bs_content_df.content,\n",
    "                                       columns=bs_tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
